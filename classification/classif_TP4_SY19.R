setwd("/Users/cecileasselin/Desktop/SY19/TD4/")
rm(list=ls())

############# CLASSIFICATION ####################

classif <- read.table(file='./Data/TPN1_a22_clas_app.txt', header = TRUE)
head(classif) # donn√©es quantitatives de X1 √† X45, donn√©es qualitatives X46 √† X50, y est la donn√©e de classe √† pr√©dire (3 classes diff√©rentes)
summary(classif)
# besoin nettoyer les donn√©es ? je ne pense pas 

###### 0. Separation des donn√©es en test et train
n <- nrow(classif)
p <- ncol(classif)-1
nb.train <- round(2*n/3) 
nb.test <- n - nb.train

# We declare the class variable as a factor (because it is a (qualitative) category variable) 
classif$y<-as.factor(classif$y)

# seed
set.seed(1729) # the Hardy‚ÄìRamanujan number

# Training/Testing data
train <- sample(1:n, nb.train) 
classif.train <- classif[train,] 
classif.test <- classif[-train,]


######## 1. ADL
# install.packages(MASS)
library(MASS)

lda.classif <- lda(y~., data=classif.train)
pred.classif.lda <- predict(lda.classif, newdata=classif.test)
summary(pred.classif.lda)

# calcul du taux d'erreur de ma pr√©diction : on est √† 0.49
err.lda <- mean(pred.classif.lda$class!=classif.test$y)

# autre mani√®re de calculer erreur empirique  
# matrix.conf.lda <- table(classif.test$y, pred.classif.lda$class) 
# matrix.conf.lda
# err.lda <- 1-sum(diag(matrix.conf.lda))/nb.test 
# err.lda


####### 2. NB 
library(naivebayes)
fit.naive <- naive_bayes(y~.,data=classif.train) 
pred.naive <- predict(fit.naive,newdata=classif.test)
err.naive <- mean(pred.naive != classif.test$y) # meilleurs r√©sultats qu'avec ADL

# ne pas oublier de v√©rifier l'hypoth√®se forte d‚Äôind√©pendance entre les pr√©dicteurs faite ac NB (i.e. ùë•ùëñ ‚üÇ ùë•ùëó)
# quel test faire ?

# Autre mani√®re de calculer l'erreur
# conf.naive <- table(classif.test$y,pred.naive) 
# conf.naive
# err.naive <- 1-sum(diag(conf.naive))/nb.test 
# err.naive

####### 3. ADQ
# on n'a peut √™tre pas assez de donn√©es pour faire une ADQ correcte (si besoin donner plus de donn√©es √† l'ensemble d'apprentissage, et faire cross-validation)
fit.qda <- qda(y~.,data=classif.train)
pred.qda <- predict(fit.qda,newdata=classif.test) 
err.qda <- mean(classif.test$y != pred.qda$class)

###### 4. Comparaison ADL / NB / ADQ ###
library(pROC)
roc.lda <- roc(classif.test$y,as.vector(pred.classif.lda$posterior[,1])) 
plot(roc.lda)

pred.nb.prob <- predict(fit.naive, newdata=classif.test, type="prob") # COMPRENDRE POURQUOI ON DOIT REFAIRE CETTE PREDICTION AVEC TYPE = PROB
roc.nb <- roc(classif.test$y, as.vector(pred.nb.prob[,1]))
plot(roc.nb, col='red') 
plot(roc.lda, add=TRUE)

# Probl√®me : je n'arrive x √† tracer ma ROC pour l'ADQ
# roc.qda <- roc(classif.test$y,as.vector(pred.qda$posterior[,1])) 
# plot(roc.qda, col='blue', add=TRUE)


##### 5.KNN
# Loading package
library(e1071)
library(caTools)
library(class)

# Feature Scaling
train_scale <- scale(classif.train[, 1:45])
test_scale <- scale(classif.test[, 1:45])

# Fitting KNN Model 
# to training dataset
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 1)
classifier_knn

# Confusion Matrix
cm <- table(classif.test$y, classifier_knn)
cm

# Model Evaluation - Choosing K
# Calculate out of Sample error
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- misClassError

# K = 3
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 3)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)

# K = 5
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 5)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)

# K = 7
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 7)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)

# K = 15
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 15)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)

# K = 19
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 19)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)


### Reste √† faire : 
# il faudrait peut √™tre normaliser nos donn√©es d√®s le d√©part (ds ts cas )pour l'instant je ne l'ai fait que pour KNN √† la fin)
# faire des tests d'hypoth√®se pour NB, ADL & ADQ
# on pourrait aussi passer √† la validation crois√©e K-fold
# essayer la regression logistique 
# selection de mod√®le: subset selec / regularizat¬∞ / feature extract¬∞ --> ACP

#### tentative subset selection (attention je ne sais pas si c'est cens√© fonctionner avec de la classification)
# install.packages("leaps")
# library(leaps)   #function to search for the best model
# reg.fit<-regsubsets(y ~.,data=classif.train) # method='exhaustive' => trop lent, & jsp si je garde nvmax ou x
# plot(reg.fit,scale="r2")