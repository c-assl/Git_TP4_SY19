#setwd("/Users/cecileasselin/Desktop/SY19/TD4/")
rm(list=ls())

############# CLASSIFICATION ####################
classif <- read.table(file='./Data/TPN1_a22_clas_app.txt', header = TRUE)
# classif <- read.table(file='TPN1_a22_clas_app.txt', header = TRUE)
head(classif) # donn??es quantitatives de X1 ?? X45, donn??es qualitatives X46 ?? X50, y est la donn??e de classe ?? pr??dire (3 classes diff??rentes)
summary(classif)

###### 0. Separation des donn??es en test et train
n <- nrow(classif)
nb.train <- round(2*n/3) 
nb.test <- n - nb.train

# We declare the class variable as a factor (because it is a (qualitative) category variable) 
classif$y<-as.factor(classif$y)
classif$y
# seed
set.seed(1729) # the Hardy?CRamanujan number

# Training/Testing data
train <- sample(1:n, nb.train) 
classif.train <- classif[train,] 
classif.test <- classif[-train,]


######## 1. ADL
# install.packages(MASS)
library(MASS)

lda.classif <- lda(y~., data=classif.train)
pred.classif.lda <- predict(lda.classif, newdata=classif.test)
summary(pred.classif.lda)

# calcul du taux d'erreur de ma pr??diction : on est ?? 0.491018
err.lda <- mean(pred.classif.lda$class!=classif.test$y)
err.lda

####### 2. NB 
library(naivebayes)
fit.naive <- naive_bayes(y~.,data=classif.train) 
pred.naive <- predict(fit.naive,newdata=classif.test)  ###Warning message:
########predict.naive_bayes(): more features in the newdata are provided as there are probability tables in the object. Calculation is performed based on features to be found in the tables. 
err.naive <- mean(pred.naive != classif.test$y) # on est ?? 0.3532934??meilleurs r??sultats qu'avec ADL

# ne pas oublier de v??rifier l'hypoth??se forte d??ind??pendance entre les pr??dicteurs faite ac NB (i.e. ???????? ??? ????????)
# quel test faire ?
##  qqPlot()Ҫ????lm()???ϡ?

####### 3. ADQ
# on n'a peut ??tre pas assez de donn??es pour faire une ADQ correcte (si besoin donner plus de donn??es ?? l'ensemble d'apprentissage, et faire cross-validation)
fit.qda <- qda(y~.,data=classif.train)
pred.qda <- predict(fit.qda,newdata=classif.test) 
err.qda <- mean(classif.test$y != pred.qda$class) #0.3473054

###### 4. Comparaison ADL / NB / ADQ ###
library(pROC)
roc.lda <- roc(classif.test$y,as.vector(pred.classif.lda$posterior[,1])) 
plot(roc.lda,col="blue")

pred.nb.prob <- predict(fit.naive, newdata=classif.test, type="prob") # COMPRENDRE POURQUOI ON DOIT REFAIRE CETTE PREDICTION AVEC TYPE = PROB
roc.nb <- roc(classif.test$y, as.vector(pred.nb.prob[,1]))
#####Warning message:
######In roc.default(classif.test$y, as.vector(pred.classif.lda$posterior[,  :
###'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead

plot(roc.nb, col='red') 
plot(roc.lda, add=TRUE)

# Probl??me : je n'arrive x ?? tracer ma ROC pour l'ADQ
roc.qda <- roc(classif.test$y,as.vector(pred.qda$posterior[,1])) 
plot(roc.qda, col='blue', add=TRUE)
legend("bottomright",legend=c("LDA","NB","QDA"),col=c("red","black","blue"),lty=1:2)

##### 5.KNN
# Loading package
library(e1071)
library(caTools)
library(class)

# Feature Scaling
train_scale <- scale(classif.train[, 1:45])
test_scale <- scale(classif.test[, 1:45])

# Fitting KNN Model 
# to training dataset
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 1)
classifier_knn

# Confusion Matrix
cm <- table(classif.test$y, classifier_knn)
cm

# Model Evaluation - Choosing K
# Calculate out of Sample error
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- misClassError

# K = 3
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 3)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)

# K = 5
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 5)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)

# K = 7
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 7)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)

# K = 15
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 15)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)

# K = 19
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = classif.train$y,
                      k = 19)
misClassError <- mean(classifier_knn != classif.test$y)
print(paste('Accuracy =', 1-misClassError))
err.knn <- min(err.knn, misClassError)

for(k in 1:30){
  classifier_knn <- knn(train = train_scale,
                        test = test_scale,
                        cl = classif.train$y,
                        k = k)
  misClassError <- mean(classifier_knn != classif.test$y)
  print(paste(k, '- Accuracy =', 1-misClassError))
  err.knn <- min(err.knn, misClassError)
  #plot(k,1-misClassError)
}
print(paste('maxAccuracy =', 1-err.knn))


### multinomial logistic regression
## https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/
## https://zhuanlan.zhihu.com/p/266664085
install.packages("nnet")
library(nnet)

#????׼??
#classif.train et classif.test
#??Ԫ????ģ?͹???
##???ӵļ????????????????Ա???refָ???ļ?????????ʾ?????༶?????ơ?
##ref : ??��???б???????֡?????顢???????���????ref??һ????��?????????ַ???????????????Ҫ??Ϊ??һ???ļ????????ƻ????ţ??ǳ??漶???Ǳ?????????ref??һ???б���??????һ??????֡????ÿ???б?Ԫ???е????Ӽ????????ϵġ??????б���???????????ƽ??????µ????Ӽ??𣬷??򽫴Ӿɵġ?????ref??һ??????֡????ά???飬??????table????????һ?о???Ψһ??x???𣬵ڶ??зֱ????иü????ķ??顣

#??????ref??????ɶ
classif.train$y<-relevel(as.factor(classif.train$y),ref="1") #ѡ???ο????࣬??????ref,ref' must be an existing level
multi.logistreg<-multinom(y~., data=classif.train)
## multinom:it does not require the data to be reshaped (as the mlogit package does) and to mirror the example code found in Hilbe??s Logistic Regression Models.
##?????????߼??ع????õ???library(nnet)???е?multinom()???????ú?????��???ص㣺һ????Ҫѡ???ο????ࣻ???ǲ??ܼ???ϵ?????????ԣ???Ҫ?Լ????㣩????Ԫ?߼??ع??У???????3?????࣬???Բο???????Ϊ?ο???????��??????ģ?͡?ģ?ͼ??????Ǹ???????????3???????ĸ??ʣ?ȡ?????????ķ???Ϊ???շ??ࡣ
summary(multi.logistreg)
####???Ͻ??????ǿ??Կ?????Ȼ???ǵ?Species????????????setosa??versicolor??virginica?????????Ͻ???ֻ??ʾ?˺?��????????ϵ????Coefficients???ͱ?׼????Std.Errors??????setosa??????ϵ???ͱ?׼????û???ڽ?????չʾ??
##??ԭ???ǣ??????߼??ع???ԭ???ǣ??ڽ?????��???????У???ѡ??һ????????Ϊ?ο?????????????????????ϵ???????????Բο???????Ϊ?ο?????????Ϊ1??????һ?????㼰????ڹ?͡???ʵ?ڶ????????߼??ع??У?????ҲӦ?õ?????һ?㣬0/1??��?У?????Ĭ?ϵ?0?ǲο???????1?Ǹ???Ȥ???𡣵??????ڶ?????????û?е??????????????Բ???Ҫ??????????ν?Ĳο??ͷǲο????𡣵??Ƕ????߼??ع?????????ͬ???ж????????ڽ?????��????Ҫ???ǲο???????ѡ????

##ϵ???????Լ???
z<-summary(multi.logistreg)$coefficients/summary(multi.logistreg)$standard.errors
pvalue<- (1 - pnorm(abs(z), 0, 1)) * 2
pvalue

#????Σ?նȣ?????Σ?շ??ձȣ?odds????OR?ȼۣ?relative risk ratio
exp(coef(multi.logistreg))

#?õ?ģ?͵?????ֵ
#  use predicted probabilities to help you understand the model. You can calculate predicted probabilities for each of our outcome levels using the fitted function. We can start by generating the predicted probabilities for the observations in our dataset and viewing the first few rows
head(pp <- fitted(multi.logistreg))

## ???Լ?????Ԥ??
pred.multi.logistreg<-predict(multi.logistreg, classif.test)
pred.multi.logistreg
pred.multi.logistreg.prob<-predict(multi.logistreg, classif.test, "prob")
pred.multi.logistreg.prob


#Ԥ????ȷ?ٷֱ?
err.logistreg <- mean(pred.multi.logistreg!=classif.test$y)
err.logistreg
# ou
matrix.multi.logistreg<-table(classif.test$y,pred.multi.logistreg)
matrix.multi.logistreg
err.logistreg<- 1-sum(diag(matrix.multi.logistreg))/nb.test
err.logistreg  #on est ?? 0.4850299

##??????��?ַ?ʽ??ʲô???????ƺ?????????
roc.logistreg <- roc(classif.test$y, as.vector(pred.multi.logistreg.prob[,1]))
roc.logistreg<-roc(classif.test$y,as.vector(pred.multi.logistreg.prob$posterior[,1])) # ne fonctionne pas pour moi
plot(roc.logistreg,col='purple',add=TRUE)

#####K-fold cross-validation
#install.packages(caret)
library(caret)
folds<-createFolds(y=classif$y,k=10)
folds
library(pROC)
max=0
num=0
auc_value<-as.numeric()
for(i in 1:10){  #10??
  fold.classif.test <- classif[folds[[i]],]# ʣ?µ???????Ϊѵ��??
  fold.classif.train <- classif[-folds[[i]],]#ȡfolds[[i]]??Ϊ???Լ?
  fold.lm <- lm(as.numeric(y)~.,data=fold.classif.train)
  fold.pred <- predict(fold.lm,type='response',newdata=fold.classif.test)
  ##AUCֵԽ????׼ȷ??Խ??
  ##??AUCֻ?????ڶ?????ģ???��?
  ## https://zhuanlan.zhihu.com/p/33407505
  #auc_value<- append(auc_value,as.numeric(auc(as.numeric(fold.classif.test[,51]),fold.pred)))
  roc.curve<-roc(as.numeric(fold.classif.test$y),fold.pred)
  plot(roc.curve,add=TRUE)
  if (i==1 | i==10){
    plot(roc.curve,add=TRUE) 
  }
}
#idx<-which.max(auc_value)
#print(auc_value)


######ajouter les variables transformees
#####??��̫????

## Ҫ??mse????????????????



###########plot_all +KNN
plot(roc.nb, col='black') 
plot(roc.lda, col='red',add=TRUE)
plot(roc.qda, col='green', add=TRUE)
plot(roc.logistreg,col='blue',add=TRUE)
legend("bottomright",legend=c("NB","LDA","QDA","Logistic Regression"),col=c("black","red","green","blue"),lty=1:2)




#######48??  2. NB 
library(naivebayes)
fit.naive <- naive_bayes(y~.,data=classif.train) 
nb.lm<-lm(y~.,data=classif.train) 
nb.lm
pred.naive <- predict(fit.naive,newdata=classif.test)  ###Warning message:
########predict.naive_bayes(): more features in the newdata are provided as there are probability tables in the object. Calculation is performed based on features to be found in the tables. 
err.naive <- mean(pred.naive != classif.test$y) # on est ?? 0.3532934??meilleurs r??sultats qu'avec ADL
## noramlite  tp2page5
##moyen1 Tracer qqnorm
##Error in qqnorm.default(resid(fit.naive)) : y is empty or has only NAs
qqnorm(resid(fit.naive))
qqline(resid(fit.naive))
resid(fit.naive) ###?в?Ϊ0??????????????????????????????Ϊʲô?в?Ϊ0
##moyen2 Tracer histogramme
hist(resid(fit.naive), freq = FALSE)
eps <- seq(-2, 2, 0.01)
lines(eps, dnorm(eps, mean=0, sd=sd(resid(fit.naive))))

shapiro.test(resid(fit.naive))   #Error in shapiro.test(fit.naive) : ???????е?is.numeric(x)????TRUE
for(i in 1:nrow(classif)){
  classif[i,]<-as.numeric(classif[i,])
}
resid(classif) #null
resid(fit.naive) #null
shapiro.test(classif[,1])      ##### ?ж?ÿһ?з?????̬?ֲ???????????



####??????û????

library(plyr)
library(reshape2)

#1??????ѵ��?????????ر?Ҷ˹??????

#1.1???????????ĸ???
##????ѵ��????D?????????ֵĸ??ʣ???P{c_i}
##???룺trainData ѵ��????????Ϊ???ݿ?
##      strClassName ָ??ѵ��????????Ϊ    strClassName??Ϊ????????
##?????????ݿ???P{c_i}?ļ??ϣ?????????|???ʣ?????Ϊ prob??
class_prob <- function(trainData, strClassName){
  #ѵ��????????
  #nrow????????
  length_train <- nrow(trainData)
  dTemp <- ddply(trainData, strClassName, "nrow")
  dTemp <- transform(dTemp, length_train=length_train) 
  dTemp <- ddply(dTemp, strClassName, mutate, prob=nrow/length_train)
  dTemp[,-c(2,3)]
}

##1.2??????ÿ???????£?????ȡ??ֵͬ?ĸ???
##????ѵ��????D??,????ÿ???????£?????ȡ??ֵͬ?ĸ??ʣ???P{fi|c_i}
##???룺trainData ѵ��????????Ϊ???ݿ?
##      strClassName ָ??ѵ��????????ΪstrClassName??Ϊ????????????????ȫ??????Ϊ??????ֵ
##?????????ݿ???P{fi|c_i}?ļ??ϣ?????????|????????|????ȡֵ|???ʣ?????Ϊ prob??
feature_class_prob <- function(trainData, strClassName){
  # ????ת??Ϊ?ݱ?
  data.melt <- melt(trainData,id=c(strClassName))
  # ͳ??Ƶ??
  aa <- ddply(data.melt, c(strClassName,"variable","value"), "nrow")
  # ????????
  bb <- ddply(aa, c(strClassName,"variable"), mutate, sum = sum(nrow), prob = nrow/sum)
  # ????????
  colnames(bb) <- c("class.name",
                    "feature.name",
                    "feature.value",
                    "feature.nrow",
                    "feature.sum",
                    "prob")
  # ???ؽ???
  bb[,c(1,2,3,6)]
}
## ???ϴ????????ر?Ҷ˹??????


## 2??ʹ?????ɵ????ر?Ҷ˹??????????Ԥ??
##ʹ?????ɵ????ر?Ҷ˹??????????Ԥ??P{fi|c_i}
##???룺oneObs ???ݿ򣬴?Ԥ????????????ʽΪ ????????|????ֵ
##      pc ???ݿ???ѵ��????D?????????ֵĸ??ʣ???P{c_i}  ????????|????
##      pfc ???ݿ???ÿ???????£?????ȡ??ֵͬ?ĸ??ʣ???P{fi|c_i}
##                  ????????|????????|????ֵ|????
##?????????ݿ򣬴?Ԥ???????ķ?????ÿ???????ĸ??ʣ?????????|???????ʣ?????Ϊ prob??
pre_class <- function(oneObs, pc,pfc){
  colnames(oneObs) <- c("feature.name", "feature.value")
  colnames(pc) <- c("class.name","prob")
  colnames(pfc) <- c("class.name","feature.name","feature.value","prob")
  
  # ȡ????????ȡֵ??????????
  feature.all <- join(oneObs,pfc,by=c("feature.name","feature.value"),type="inner")
  # ȡ??????ȡֵ??????????��??
  feature.prob <- ddply(feature.all,.(class.name),summarize,prob_fea=prod(prob))  #prodΪ��?˺???
  
  #ȡ???????ĸ???
  class.all <- join(feature.prob,pc,by="class.name",type="inner")
  #????????
  ddply(class.all,.(class.name),mutate,pre_prob=prob_fea*prob)[,c(1,4)]
}


##3?????ݲ???1
##??????ƻ??????????Ϊ???ӽ??в???
#ѵ��??
train.apple <- data.frame(
  size=c("??","С","??","??","С","С"),
  weight=c("??","??","??","??","??","??"),
  color=c("??","??","??","??","??","??"),
  taste=c("good","good","bad","bad","bad","good")
)
#??Ԥ??????
oneObs <- data.frame(
  feature.name =c("size", "weight", "color"),
  feature.value =c("??","??","??")
)

#Ԥ??????
pc <- class_prob(train.apple, "taste")
pfc <- feature_class_prob(train.apple, "taste")
pre_class(oneObs, pc, pfc)

