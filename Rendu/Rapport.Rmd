---
title: "Rapport TP4 SY19 GR D2P1G"
output:
  pdf_document: 
    latex_engine: xelatex
  html_notebook: default
---

**Rapport de projet 1 SY19 A22 - Sun Jingwen, Ho Nhu Hoang, Asselin Cecile**\
\
**Sommaire**

I. Introduction

II\. Regression

1.  Exploration et pr√©paration des donn√©es
2.  Choix d'un mod√®le de regression et optimisation de ce mod√®le
3.  Analyse des r√©sultats

III\. Classification

1.  Exploration et pr√©paration des donn√©es
2.  Choix d'un mod√®le de classification et optimisation de ce mod√®le
3.  Analyse des r√©sultats

I. Introduction

Nous chargeons les diff√©rents packages et les donn√©es n√©cessaires √† l'√©tude.

```{r, echo=FALSE}
#Regression
library(gridExtra)
library(ggplot2)
library(tidyverse)
library(caret)
library(leaps)
library(MASS)
library(glmnet)
library(gam)

reg.data <- read.table('../data/TPN1_a22_reg_app.txt', header=TRUE)
classif <- read.table('../data/TPN1_a22_clas_app.txt', header=TRUE)
```

Nous pouvons analyser les donn√©es de r√©gression et les donn√©es de classification respectivement contenues dans la variable <code>reg.data</code> et la variable <code>classif</code>.

<u>I. ***REGRESSION:***</u>

**A.** **Exploration et pr√©paration des donn√©e**

La premi√®re √©tape de l'√©tude de r√©gression consiste √† explorer les donn√©es.

```{r}
summary.data = as.data.frame(apply(reg.data, 2, summary))
summary.data
n = nrow(reg.data)
p = ncol(reg.data) - 1
matplot(t(summary.data[3,-length(summary.data)]),
        type = "l",
        main = "Median",
        ylab = "value")

```

Nous regardons que: - La taille de l'√©chantillon n = 500 et le nombre de pr√©dicteurs : p = 100. Nous en d√©duisons donc que notre jeu de donn√©es est de grande dimension et n\>p, ce qui sera fondamental dans la partie mod√®le. - En regardant les statistiques r√©capitulatives (<code>summary.data</code>) des diff√©rents pr√©dicteurs, nous comprenons que tous ont une valeur m√©diane comprise entre 4,7 et 5,29 avec une valeur √©chelonn√©e de 0 √† 10. Ils ont donc distributions identiques, il n'est donc pas n√©cessaire de mettre √† l'√©chelle le pr√©dicteur de donn√©es.

**1. Pr√©paration de donn√©e** Dans un premier temps, nous √©valuerons rapidement l'ensemble du mod√®le dans sa complexit√©, sans chercher √† l'am√©liorer ou √† le simplifier, en appliquant une simple r√©gression lin√©aire. Et pour ce faire, il faut pr√©parer les donn√©es en les divisant al√©atoirement en un ensemble d'apprentissage (66% pour construire un mod√®le pr√©dictif) et un ensemble de validation (34% pour √©valuer le mod√®le).

```{r}
########### training set and validation set ################
set.seed(1729) # the Hardy CRamanujan number

rows <- nrow(reg.data)
cols <- ncol(reg.data) - 1
train_size <- 2 / 3
nb_train <- round(train_size * rows)
nb_test <- rows - nb_train

# training set
train <- sample(1:rows, nb_train)
reg.data.train <- reg.data[train,]
# validation set
reg.data.test  <- reg.data[-train,]
```

\*Nous cr√©ons une fonction pour obtenir l'erreur quadratique moyenne des diff√©rents mod√®les que nous allons √©valuer et comparer pour trouver le meilleur : <u>MSE</u>. 2 arguments : -y_predict¬†: le y pr√©dit par le mod√®le avec l'observation de test. -y_test : la r√©ponse y des donn√©es de test correspondant aux observations de test.

```{r}
# MSE
MSE = function(y_test,y_predict){
  mean((y_test-y_predict)^2)
}
```

```{r}
full.model.reg <- lm(y~., data = reg.data.train)
summary(full.model.reg)
predictions <- predict(full.model.reg, newdata = reg.data.test)
full.model.reg.mse <- MSE(reg.data.test$y, predictions)
plot(reg.data.test$y, predictions)
abline(0,1)
full.model.reg.mse
```

Cette premi√®re √©tude montre que le mod√®le tel qu'il inclut tous les pr√©dicteurs : Xi i = 1,..., 100 g√©n√®re d√©j√† une bonne r√©gression lin√©aire multidimensionnelle pour pr√©dire y. L'id√©e est maintenant de d√©terminer un meilleur mod√®le pour obtenir la meilleure r√©gression possible. Il est important de pr√©ciser que pour la comparaison de nos mod√®les mais aussi l'estimation des hyperparam√®tres de certains mod√®les, nous utiliserons principalement la kfold-cross-validation (k-CV). En ce qui concerne la s√©lection du meilleur mod√®le, nous avons vu que le meilleur mod√®le est d√©fini comme le mod√®le avec l'erreur de pr√©diction la plus faible (RMSE) et, par cons√©quent, la MSE la plus faible. Ainsi, le k-CV, une m√©thode "d'estimation d'erreur directe", a un avantage sur l'AIC, le BIC et le R2 ajust√© vus en classe, en ce qu'il fournit une estimation directe de l'erreur de pr√©diction. En outre, il peut √©galement √™tre utilis√© dans un plus large √©ventail de t√¢ches de s√©lection de mod√®les. Comme chaque ensemble d'apprentissage est seulement (K - 1)/K aussi grand que l'ensemble d'apprentissage d'origine, les estimations d'erreur de pr√©diction seront g√©n√©ralement biais√©es vers le haut. Ce biais est minimis√© lorsque K = n (LOOCV), mais cette estimation a une variance √©lev√©e, car les estimations pour chaque pli sont fortement corr√©l√©es. K = 5, compte tenu de notre grande taille d'√©chantillon n, fournit un bon compromis pour ce compromis biais-variance.

```{r}
train.control <- trainControl(method = "cv", number = 5)
```

Passons maintenant √† la s√©lection du mod√®le : Pour cela, nous allons passer en revue toutes les m√©thodes vues en cours pour trouver les meilleurs mod√®les et les tester pour voir s'ils ¬´ correspondent ¬ª √† notre jeu de donn√©es. Pour cela, nous allons g√©n√©rer un 5-CV en utilisant toutes les donn√©es fournies, dans lequel √† chaque it√©ration, k : Pour chaque m√©thode de d√©duction du mod√®le optimis√©¬†: -Nous allons d'abord estimer les param√®tres de r√©glage du mod√®le associ√© √† la m√©thode actuelle en utilisant tous les plis sauf le pli k. -A partir de l√†, nous obtiendrons les meilleurs param√®tres de r√©glage associ√©s √† la m√©thode actuelle. -Enfin, nous allons r√©estimer le mod√®le en utilisant tous les plis sauf le pli k, et nous l'√©valuons avec le pli k en calculant son MSE, que nous stockons dans la table des cv_errors correspondant √† la m√©thode courante.

Enfin, nous allons calculer le cv_error moyen pour chaque mod√®le obtenu et s√©lectionner celui avec le plus petit cv_error.

Cette proc√©dure est, √† notre avis, celle qui offre les r√©sultats les moins biais√©s.

*\*important note:* La m√©thode de s√©lection du meilleur sous-ensemble est tr√®s puissante mais, pour des raisons de calcul, elle ne peut pas √™tre appliqu√©e ici car p est trop grand. De plus, la s√©lection du meilleur sous-ensemble, m√™me si elle √©tait possible ici, peut √©galement souffrir de probl√®mes statistiques lorsque p est grand¬†: plus l'espace de recherche est grand, plus il est probable de trouver des mod√®les qui semblent bons sur les donn√©es d'apprentissage, m√™me s'ils ont aucun pouvoir pr√©dictif sur les donn√©es futures. Ainsi, un espace de recherche √©norme peut conduire √† un surajustement et √† une variance √©lev√©e des estimations de coefficients. Pour ces deux raisons, les m√©thodes pas √† pas explorant un ensemble beaucoup plus restreint de mod√®les repr√©sentent ici de tr√®s bonnes alternatives √† la meilleure s√©lection de sous-ensemble que nous allons donc mettre en ≈ìuvre. De plus, apr√®s avoir vu en classe que les m√©thodes des plus proches voisins peuvent mal fonctionner lorsque p est grand car les plus proches voisins ont tendance √† √™tre √©loign√©s en grandes dimensions, nous avons d√©cid√© de ne pas les utiliser dans le CV suivant en raison du grand nombre de pr√©dicteurs disponibles ici.

**2. Choix d'un mod√®le et l'optimisation** On utilise les m√©thodes suivants: Stepwise selection: <u>1 Forward selection</u> <u>2 Backward selection</u>

Penelized regression: <u>1 Ridge regression</u> <u>2 Lasso regression</u> <u>3 Elasticnet regression</u>

```{r}
# 10-CV set up:
kfolds <- 10
n <- nrow(reg.data)
p <- ncol(reg.data)
ntst <- n/kfolds

## Define fold_ids
fold_ids      <- rep(seq(kfolds), ceiling(n / kfolds))
fold_ids      <- fold_ids[1:n]
fold_ids      <- sample(fold_ids, length(fold_ids))

## Initialize vectors to store CV errors
Elastic_CV_MSE_vec  <- vector(length = kfolds, mode = "numeric")
Lasso_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Ridge_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Forward_step_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Backward_step_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Full_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")

#10-CV execution
## Loop through the folds:
for (k in 1:kfolds){

  ############## STEPWISE SELECTION #############

  forward.step.model <- train(y ~., data = reg.data[which(fold_ids != k),],
                            method = "leapForward",
                            tuneGrid = data.frame(nvmax = 1:100),
                            trControl = train.control
  )
  step.predictor.names <- names(coef(forward.step.model$finalModel, forward.step.model$bestTune$nvmax))[1:forward.step.model$bestTune$nvmax+1]

  step.model.linreg = lm(paste("y","~", paste(step.predictor.names, collapse=" + ") ), data = reg.data[which(fold_ids != k),])
  predictions <- predict(step.model.linreg,newdata=reg.data[which(fold_ids == k),])

  Forward_step_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  backward.step.model <- train(y ~., data = reg.data[which(fold_ids != k),],
                            method = "leapBackward",
                            tuneGrid = data.frame(nvmax = 1:100),
                            trControl = train.control
  )
  step.predictor.names <- names(coef(backward.step.model$finalModel, backward.step.model$bestTune$nvmax))[1:backward.step.model$bestTune$nvmax+1]

  step.model.linreg = lm(paste("y","~", paste(step.predictor.names, collapse=" + ") ), data = reg.data[which(fold_ids != k),])
  predictions <- predict(step.model.linreg,newdata=reg.data[which(fold_ids == k),])

  Backward_step_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  ############# PENALIZED REGRESSION ###########
  x <- model.matrix(y~., reg.data[which(fold_ids != k),-p-1])[,-1]

  y <- reg.data[which(fold_ids != k),]$y
  x.test <- model.matrix(y~., reg.data[which(fold_ids == k),-p-1])[,-1]

  ########## ELASTICNET REGRESSION #############
  elastic <- train(y ~., data = reg.data[which(fold_ids != k),],
                   method = "glmnet",
                   trControl = train.control,
                   tuneLength = 10)

  elastic.alpha <- elastic$bestTune[1,1]
  elastic.lambda <- elastic$bestTune[1,2]

  fit.elsatic <- glmnet(x, reg.data[which(fold_ids != k),p], lambda=elastic.lambda, alpha = elastic.alpha)
  predictions  <- predict(fit.elsatic, x.test)

  Elastic_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  ########## RIDGE REGRESSION #############
  cv.ridge <- cv.glmnet(x, y, alpha = 0)

  fit.ridge <- glmnet(x, reg.data[which(fold_ids != k),p], lambda=cv.ridge$lambda.min, alpha = 0)
  predictions  <- predict(fit.ridge, x.test)
  Ridge_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  ########## LASSO REGRESSION #############
  cv.lasso <- cv.glmnet(x, y, alpha = 1)
  fit.lasso <- glmnet(x, reg.data[which(fold_ids != k),p], lambda=cv.lasso$lambda.min, alpha = 1)
  predictions  <- predict(fit.lasso, x.test)
  Lasso_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  #REGRESSION OBTAINED W/ THE FULL MODEL (NO SELECTION)
  model.linreg = lm(y~.,  data = reg.data[which(fold_ids != k),])
  predictions <- predict(model.linreg,newdata=reg.data[which(fold_ids == k),])

  Full_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

}
```

Nous sommes maintenant en mesure de comparer les r√©sultats des performances de notre mod√®le.

```{r}
noquote(sprintf("forward step regression model MSE: %.3f",mean(Forward_step_CV_MSE_vec)))
noquote(sprintf("backward step regression MSE: %.3f",mean(Backward_step_CV_MSE_vec)))
noquote(sprintf("elastic net model MSE: %.3f",mean(Elastic_CV_MSE_vec)))
noquote(sprintf("lasso net model MSE: %.3f",mean(Lasso_CV_MSE_vec)))
noquote(sprintf("ridge model MSE: %.3f",mean(Ridge_CV_MSE_vec)))
noquote(sprintf("full model regression MSE: %.3f",mean(Full_CV_MSE_vec)))
```

Nous en d√©duisons que parmi tous les travaux que nous avons effectu√©s, la s√©lection pas √† pas pr√©c√©dente donne le mod√®le avec le meilleur RMSE en toutes circonstances. Ce mod√®le pas √† pas devient alors notre meilleur mod√®le.

**Analyse des r√©sultats** La derni√®re √©tape de cette √©tude de r√©gression sera d'analyser et d'√©valuer le mod√®le de r√©gression que nous pouvons obtenir avec le meilleur mod√®le trouv√©¬†: Pour obtenir l'erreur de test la moins biais√©e pour notre meilleur mod√®le de pr√©diction, nous proc√©derons comme suit¬†: nous r√©estimerons les param√®tres du mod√®le √† l'aide de l'ensemble d'apprentissage. Et pour obtenir une estimation impartiale de la meilleure erreur de mod√®le, nous utiliserons notre ensemble de validation cr√©√© au d√©but comme ensemble de test ind√©pendant.

\*Nous avons cr√©√© des fonctions pour analyser et √©valuer le meilleur mod√®le que nous avons trouv√© pour la r√©gression¬†: - <u>modelDiagnostic¬†:</u> Cette fonction impl√©mente 3 trac√©s r√©siduels utiles par rapport √† notre mod√®le¬†: -\> le premier graphique repr√©sente les r√©sidus par rapport aux valeurs pr√©dites pour l'ensemble de donn√©es. La ligne bleue est un ajustement en douceur aux r√©sidus, destin√© √† cr√©er une tendance. -\> le deuxi√®me graphique d√©termine si les r√©sidus sont autour de z√©ro pour la plage de valeurs ajust√©es. -\> le dernier graphique (graphique Q-Q des r√©sidus) montre si les r√©sidus suivent une distribution normale. - <u>modelObservations¬†:</u> cette fonction repr√©sente graphiquement la pr√©diction et l'intervalle de pr√©diction en fonction des valeurs de test. elle renvoie le pourcentage de la nouvelle observation dans l'intervalle de pr√©diction.

```{r}
modelDiagnostic <- function(model.reg)
{
  pMod <- fortify(model.reg)

  # the residuals vs predicted values
  p1 <- ggplot(pMod, aes(x=.fitted, y=.resid))+geom_point() +
    geom_smooth(se=FALSE)+geom_hline(yintercept=0, col="red", linetype="dashed") +
    xlab("Fitted Values")+ylab("Residuals") +
    ggtitle("Residual vs Fitted Plot")

  pMod$.qqnorm <- qqnorm(pMod$.stdresid, plot.it=FALSE)$x
  y <- quantile(pMod$.stdresid, c(0.25, 0.75))   # Find the 1st and 3rd quartiles
  x <- quantile(pMod$.qqnorm  , c(0.25, 0.75))   # Find the 1st and 3rd quartiles
  slope <- diff(y) / diff(x)                     # Compute the line slope
  int <- y[1] - slope * x[1]                     # Compute the line intercept

  # Create residuals QQ plot.
  p2 <- ggplot(pMod, aes(.qqnorm, .stdresid)) +
    geom_point(na.rm = TRUE) +
    geom_abline(intercept=int, slope=slope, color="red")           +
    xlab("Theoretical Quantiles") + ylab("Standardized Residuals") +
    ggtitle("Normal Q-Q Plot")

  # Create residuals histogram plot.
  p3 <- ggplot(data=pMod, aes(x=.resid)) +
    geom_histogram(binwidth=0.5, fill="blue") +
    xlab("Residuals") +
    ggtitle("Distribution of Residuals")

  grid.arrange(p1, p3, p2, nrow = 1 , top="Model Diagnostic Plots")
}

modelObservations <- function(model.reg, data.test){
  #plotting ytest and ypred
  ypred <- predict(model.reg,data.test)
  plot(data.test$y,ypred, main="predicted values versus test values",
       xlab="predicted y",
       ylab="test y")
  abline(0,1)

  # getting the observations outside the prediction interval
  ic.error.bar <- function(x, lower, upper, length=0.1){
    arrows(x, upper, x, lower, angle=90, code=3, length=length)
  }

  #Calculate the prediction intervals of ùë¶0 (the predicted value), with the predict command using the interval = "prediction" option
  pred.test.ip <- predict(model.reg, interval='prediction', newdata = data.test)
  ytest <- data.test$y

  # get the observations being outside the prediction interval
  idx.obs.out.ip <- which((ytest<pred.test.ip[,2])|(ytest>pred.test.ip[,3]))

  # Plotting the graph with the prediction intervals.
  plot(pred.test.ip[, 1], ytest, pch=19, col='blue', ylim=range(pred.test.ip, ytest),
       main=" predicted values and the 95% PI versus the observed values.",
       xlab="predicted y",
       ylab="test y")
  abline(0,1)
  ic.error.bar(pred.test.ip[, 1], pred.test.ip[, 2], pred.test.ip[, 3])
  points(pred.test.ip[idx.obs.out.ip, 1], ytest[idx.obs.out.ip], pch=19, col="red")

  print(paste("number of observed values (ùë¶ùëñ) outside the prediction interval among ", length(ytest), " observed values in total: ", length(idx.obs.out.ip)))
  #returns the percentage that the new observation is in the prediction interval
  1 - (length(idx.obs.out.ip)/length(ytest))
}
```

```{r}
# Best model
forward.step.model <- train(y ~., data = reg.data.train,
                          method = "leapForward",
                          tuneGrid = data.frame(nvmax = 1:100),
                          trControl = train.control
)

step.predictor.names <- names(coef(forward.step.model$finalModel, forward.step.model$bestTune$nvmax))[1:forward.step.model$bestTune$nvmax+1]

step.model.linreg = lm(paste("y","~", paste(step.predictor.names, collapse=" + ") ), data = reg.data.train)
summary(step.model.linreg)
ypred <- predict(step.model.linreg,newdata=reg.data.test)

noquote(sprintf("Best model MSE: %.3f", MSE(reg.data.test$y,ypred)))

modelDiagnostic(step.model.linreg)
P <- modelObservations(step.model.linreg, reg.data.test)
```

Premi√®rement, <code>summary(step.model.linreg)</code> nous montre que la plupart de nos pr√©dicteurs s√©lectionn√©s sont significatifs et que, selon la p-value (=2.2e-16 \<0.05 =\> nous rejetons le H0 hypoth√®se que tous les coefficients b√™ta j sont √©gaux √† 0), notre mod√®le est globalement significatif pour expliquer la r√©ponse variable y. Deuxi√®mement, les diagrammes de diagnostic du mod√®le ci-dessus montrent que le mod√®le est passable. Il y a une bonne dispersion des r√©sidus autour de z√©ro pour la plage des valeurs ajust√©es (la valeur moyenne des r√©sidus est, en fait, nulle). De plus, le graphique Q-Q des r√©sidus et l'histogramme montrent une distribution plut√¥t normale. Troisi√®mement, nous observons que les intervalles de pr√©diction sont assez larges car l'intervalle de pr√©diction ùë¶0 prend en compte l'incertitude autour de lui (la variable al√©atoire) et la pr√©diction moyenne. De plus, nous pouvons voir tr√®s peu de valeurs observ√©es (ùë¶ùëñ) en dehors de l'intervalle de pr√©diction (en point rouge). Enfin, le pourcentage que la nouvelle observation se trouve dans l'intervalle de pr√©diction est¬†:

```{r}
P * 100
```

Nous pouvons donc conclure que notre meilleur mod√®le est passable et que sa pr√©diction est tr√®s bonne pour un jeu de donn√©es de test ind√©pendant du jeu de donn√©es qui a √©t√© utilis√© pour construire et entra√Æner notre mod√®le. Nous sommes donc satisfaits de notre √©tude et pr√™ts √† utiliser notre meilleur mod√®le pour le jour des √©valuations.

**II. Classification**

**1. Pr√©sentation des donn√©es** **2. Choix d'un mod√®le de classification et optimisation de ce mod√®le 3. Analyse des r√©sultats**

```{r}
# code KNN
# library(nnet)
# fit<-multinom(y~., data=classif.train)
# pred.classif<-predict(fit, newdata=classif.test)
# perf<-table(classif.test$y, pred.classif)
# err.reglog <- 1-sum(diag(perf))/nb.test
```

Test d'insertion d'image ![Ceci est un test](/Users/cecileasselin/Desktop/SY19/TD4/test_plot.png)

```{r}
# head()
```

```{r setup}
library(r2d3)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
