---
title: "Rapport TP4 SY19 GR D2P1G"
output:
  pdf_document:
    latex_engine: xelatex
  html_notebook: default
  html_document:
    df_print: paged
---

**Rapport de projet 1 SY19 A22 - Sun Jingwen, Ho Nhu Hoang, Asselin Cecile**\
\
**Sommaire**

I. Regression

1.  Exploration et pr√©paration des donn√©es
2.  Choix d'un mod√®le de regression et optimisation de ce mod√®le
3.  Analyse des r√©sultats

II\. Classification

1.  Exploration et pr√©paration des donn√©es
2.  Choix d'un mod√®le de classification et optimisation de ce mod√®le
3.  Analyse des r√©sultats

```{r message=TRUE, warning=FALSE, include=FALSE}
library(gridExtra)
library(ggplot2)
library(tidyverse)
library(caret)
library(leaps)
library(MASS)
library(glmnet)
library(gam)
library(rpart)
library(rpart.plot)
library(e1071)
library(caTools)
library(class)
library(klaR)
library(naivebayes)
library(nnet)
library(pROC)

reg.data <- read.table('../data/TPN1_a22_reg_app.txt', header=TRUE)
# reg.data <- read.table('TPN1_a22_reg_app.txt', header=TRUE)
```

<u>I. **REGRESSION**</u>

**A.** **Exploration et pr√©paration des donn√©es**

Nous commen√ßons par explorer les donn√©es.

```{r include=FALSE, warning=FALSE}
summary.data = as.data.frame(apply(reg.data, 2, summary))
n <- nrow(reg.data)
p <- ncol(reg.data) - 1
matplot(t(summary.data[3,-length(summary.data)]),type = "l", main = "Median", ylab = "value")
```

-   La taille de l'√©chantillon est n = 500 et le nombre de pr√©dicteurs p = 100. Notre jeu de donn√©es est donc de grande dimension (n\>p), ce qui sera fondamental dans la partie mod√®le.

```{r echo=TRUE, warning=FALSE}
summary.data <- as.data.frame(apply(reg.data, 2, summary))
```

-   En regardant les statistiques r√©capitulatives des diff√©rents pr√©dicteurs, nous comprenons que tous ont une valeur m√©diane comprise entre 4,7 et 5,29 avec une valeur √©chelonn√©e de 0 √† 10. Ils ont donc distributions identiques, il n'est donc pas n√©cessaire de mettre √† l'√©chelle le pr√©dicteur de donn√©es.

Dans un premier temps, nous √©valuerons rapidement l'ensemble du mod√®le dans sa complexit√©, sans chercher √† l'am√©liorer ou √† le simplifier, en appliquant une simple r√©gression lin√©aire apr√®s la s√©paration des donn√©es en train et en test.

```{r include=FALSE, warning=FALSE}
########### training set and validation set ################
set.seed(1729) # the Hardy CRamanujan number
rows <- nrow(reg.data)
cols <- ncol(reg.data) - 1
train_size <- 2 / 3
nb_train <- round(train_size * rows)
nb_test <- rows - nb_train
# training set
train <- sample(1:rows, nb_train)
reg.data.train <- reg.data[train,]
# validation set
reg.data.test  <- reg.data[-train,]
```

Nous cr√©ons une fonction pour obtenir l'erreur quadratique moyenne des diff√©rents mod√®les que nous allons √©valuer et comparer pour trouver le meilleur : <u>MSE</u>=<code>mean((y_test-y_predict)\^2)</code>.

```{r include=FALSE, warning=FALSE}
MSE = function(y_test,y_predict){
  mean((y_test-y_predict)^2)
}
```

```{r,echo=FALSE, warning=FALSE}
full.model.reg <- lm(y~., data = reg.data.train)
#summary(full.model.reg)
predictions <- predict(full.model.reg, newdata = reg.data.test)
full.model.reg.mse <- MSE(reg.data.test$y, predictions)
plot(reg.data.test$y, predictions)
abline(0,1)
sprintf("full.model.reg.mse = %3f",full.model.reg.mse)
```

Cette premi√®re √©tude montre que le mod√®le tel qu'il inclut tous les pr√©dicteurs : Xi i = 1,..., 100 g√©n√®re d√©j√† une bonne r√©gression lin√©aire multidimensionnelle pour pr√©dire y. L'id√©e est maintenant de d√©terminer un meilleur mod√®le pour obtenir la meilleure r√©gression possible.

```{r include=FALSE, warning=FALSE}
train.control <- trainControl(method = "cv", number = 5)
```

**B. Comparaison des diff√©rentes m√©thodes utlis√©es**

On utilise les m√©thodes suivantes avec la k-fold cross-validation o√π k=10 :

-   Stepwise selection: <u>1 Forward selection</u> <u>2 Backward selection</u>

-   Penalized regression: <u>1 Ridge regression</u> <u>2 Lasso regression</u> <u>3 Elasticnet regression</u>

Nous avons pens√© de prime abord que la m√©thode de s√©lection du meilleur sous-ensemble de pr√©dicteurs serait tr√®s efficace mais, pour des raisons de calcul, elle ne peut pas √™tre appliqu√©e ici car p est trop grand. De plus, m√™me si la s√©lection du meilleur sous-ensemble √©tait possible, peut √©galement souffrir de probl√®mes statistiques lorsque p est grand¬†: plus l'espace de recherche est grand, plus il est probable de trouver des mod√®les qui semblent bons sur les donn√©es d'apprentissage, m√™me s'ils n'ont aucun pouvoir pr√©dictif sur les donn√©es futures. Ainsi, un espace de recherche √©norme peut conduire √† un surapprentissage et √† une variance √©lev√©e des estimations des coefficients.

Pour ces deux raisons, les m√©thodes pas √† pas explorant un ensemble beaucoup plus restreint de mod√®les repr√©sentent ici de tr√®s bonnes alternatives √† la meilleure s√©lection de sous-ensemble que nous allons donc mettre en ≈ìuvre.

De plus, apr√®s avoir vu en classe que les m√©thodes des plus proches voisins peuvent mal fonctionner lorsque p est grand car les plus proches voisins ont tendance √† √™tre √©loign√©s en grandes dimensions, nous avons d√©cid√© de ne pas les utiliser dans la cross-validation suivante en raison du grand nombre de pr√©dicteurs disponibles ici.

En outre, la cross-validation peut √©galement √™tre utilis√©e dans un plus large √©ventail de t√¢ches de s√©lection de mod√®les. Comme chaque ensemble d'apprentissage est seulement (K - 1)/K aussi grand que l'ensemble d'apprentissage d'origine, les estimations d'erreur de pr√©diction seront g√©n√©ralement biais√©es vers le haut. Ce biais est minimis√© lorsque K = n (LOOCV), mais cette estimation a une variance √©lev√©e, car les estimations pour chaque pli sont fortement corr√©l√©es. K = 5, compte tenu de notre grande taille d'√©chantillon n, fournit un bon compromis pour ce compromis biais-variance.

Passons maintenant √† la s√©lection du mod√®le : nous allons passer en revue toutes les m√©thodes vues en cours pour trouver les meilleurs mod√®les et les tester pour voir s'ils ¬´ correspondent ¬ª √† notre jeu de donn√©es. Pour chaque m√©thode de d√©duction du mod√®le optimis√©¬†:\
- Nous allons d'abord estimer les param√®tres de r√©glage du mod√®le associ√© √† la m√©thode actuelle en utilisant tous les plis sauf le pli k.\
- A partir de l√†, nous obtiendrons les meilleurs param√®tres de r√©glage associ√©s √† la m√©thode actuelle.\
- Enfin, nous allons r√©estimer le mod√®le en utilisant tous les plis sauf le pli k, et nous l'√©valuons avec le pli k en calculant son MSE, que nous stockons dans la table des cv_errors correspondant √† la m√©thode courante.

```{r,echo=FALSE, warning=FALSE}
# 10-CV set up:
kfolds <- 10
n <- nrow(reg.data)
p <- ncol(reg.data)
ntst <- n/kfolds

## Define fold_ids
fold_ids      <- rep(seq(kfolds), ceiling(n / kfolds))
fold_ids      <- fold_ids[1:n]
fold_ids      <- sample(fold_ids, length(fold_ids))

## Initialize vectors to store CV errors
Elastic_CV_MSE_vec  <- vector(length = kfolds, mode = "numeric")
Lasso_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Ridge_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Forward_step_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Backward_step_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Full_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")

```

```{r,echo=FALSE, warning=FALSE}
for (k in 1:kfolds){
  ############## STEPWISE SELECTION #############

  forward.step.model <- train(y ~., data = reg.data[which(fold_ids != k),], method = "leapForward", tuneGrid = data.frame(nvmax = 1:100), trControl = train.control)
  step.predictor.names <- names(coef(forward.step.model$finalModel, forward.step.model$bestTune$nvmax))[1:forward.step.model$bestTune$nvmax+1]

  step.model.linreg = lm(paste("y","~", paste(step.predictor.names, collapse=" + ") ), data = reg.data[which(fold_ids != k),])
  predictions <- predict(step.model.linreg,newdata=reg.data[which(fold_ids == k),])

  Forward_step_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  backward.step.model <- train(y ~., data = reg.data[which(fold_ids != k),],
                            method = "leapBackward",
                            tuneGrid = data.frame(nvmax = 1:100),
                            trControl = train.control
  )
  step.predictor.names <- names(coef(backward.step.model$finalModel, backward.step.model$bestTune$nvmax))[1:backward.step.model$bestTune$nvmax+1]

  step.model.linreg = lm(paste("y","~", paste(step.predictor.names, collapse=" + ") ), data = reg.data[which(fold_ids != k),])
  predictions <- predict(step.model.linreg,newdata=reg.data[which(fold_ids == k),])

  Backward_step_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  ############# PENALIZED REGRESSION ###########
  x <- model.matrix(y~., reg.data[which(fold_ids != k),-p-1])[,-1]

  y <- reg.data[which(fold_ids != k),]$y
  x.test <- model.matrix(y~., reg.data[which(fold_ids == k),-p-1])[,-1]

  ########## ELASTICNET REGRESSION #############
  elastic <- train(y ~., data = reg.data[which(fold_ids != k),],
                   method = "glmnet",
                   trControl = train.control,
                   tuneLength = 10)

  elastic.alpha <- elastic$bestTune[1,1]
  elastic.lambda <- elastic$bestTune[1,2]

  fit.elsatic <- glmnet(x, reg.data[which(fold_ids != k),p], lambda=elastic.lambda, alpha = elastic.alpha)
  predictions  <- predict(fit.elsatic, x.test)

  Elastic_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  ########## RIDGE REGRESSION #############
  cv.ridge <- cv.glmnet(x, y, alpha = 0)

  fit.ridge <- glmnet(x, reg.data[which(fold_ids != k),p], lambda=cv.ridge$lambda.min, alpha = 0)
  predictions  <- predict(fit.ridge, x.test)
  Ridge_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  ########## LASSO REGRESSION #############
  cv.lasso <- cv.glmnet(x, y, alpha = 1)
  fit.lasso <- glmnet(x, reg.data[which(fold_ids != k),p], lambda=cv.lasso$lambda.min, alpha = 1)
  predictions  <- predict(fit.lasso, x.test)
  Lasso_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  #REGRESSION OBTAINED W/ THE FULL MODEL (NO SELECTION)
  model.linreg = lm(y~.,  data = reg.data[which(fold_ids != k),])
  predictions <- predict(model.linreg,newdata=reg.data[which(fold_ids == k),])

  Full_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)
}
```

Nous sommes maintenant en mesure de comparer les r√©sultats des performances de chaque m√©thode.

```{r echo=FALSE, warning=FALSE}
noquote(sprintf("forward step regression model MSE: %.3f",mean(Forward_step_CV_MSE_vec)))
noquote(sprintf("backward step regression MSE: %.3f",mean(Backward_step_CV_MSE_vec)))
noquote(sprintf("elastic net model MSE: %.3f",mean(Elastic_CV_MSE_vec)))
noquote(sprintf("lasso net model MSE: %.3f",mean(Lasso_CV_MSE_vec)))
noquote(sprintf("ridge model MSE: %.3f",mean(Ridge_CV_MSE_vec)))
noquote(sprintf("full model regression MSE: %.3f",mean(Full_CV_MSE_vec)))
```

**C. Analyse des r√©sultats**

```{r echo=FALSE, warning=FALSE}
modelDiagnostic <- function(model.reg){
  pMod <- fortify(model.reg)

  # the residuals vs predicted values
  p1 <- ggplot(pMod, aes(x=.fitted, y=.resid))+geom_point() +
    geom_smooth(se=FALSE)+geom_hline(yintercept=0, col="red", linetype="dashed") +
    xlab("Fitted Values")+ylab("Residuals") +
    ggtitle("Residual vs Fitted Plot")

  pMod$.qqnorm <- qqnorm(pMod$.stdresid, plot.it=FALSE)$x
  y <- quantile(pMod$.stdresid, c(0.25, 0.75))   # Find the 1st and 3rd quartiles
  x <- quantile(pMod$.qqnorm  , c(0.25, 0.75))   # Find the 1st and 3rd quartiles
  slope <- diff(y) / diff(x)                     # Compute the line slope
  int <- y[1] - slope * x[1]                     # Compute the line intercept

  # Create residuals QQ plot.
  p2 <- ggplot(pMod, aes(.qqnorm, .stdresid)) +
    geom_point(na.rm = TRUE) +
    geom_abline(intercept=int, slope=slope, color="red")           +
    xlab("Theoretical Quantiles") + ylab("Standardized Residuals") +
    ggtitle("Normal Q-Q Plot")

  # Create residuals histogram plot.
  p3 <- ggplot(data=pMod, aes(x=.resid)) +
    geom_histogram(binwidth=0.5, fill="blue") + xlab("Residuals") + ggtitle("Distribution of Residuals")

  grid.arrange(p1, p3, p2, nrow = 1 , top="Model Diagnostic Plots")
}

modelObservations <- function(model.reg, data.test){
  #plotting ytest and ypred
  ypred <- predict(model.reg,data.test)
  #plot(data.test$y,ypred, main="predicted values versus test values", xlab="predicted y", ylab="test y")
  #abline(0,1)

  # getting the observations outside the prediction interval
  ic.error.bar <- function(x, lower, upper, length=0.1){
    arrows(x, upper, x, lower, angle=90, code=3, length=length)
  }

  #Calculate the prediction intervals of ùë¶0 (the predicted value), with the predict command using the interval = "prediction" option
  pred.test.ip <- predict(model.reg, interval='prediction', newdata = data.test)
  ytest <- data.test$y

  # get the observations being outside the prediction interval
  idx.obs.out.ip <- which((ytest<pred.test.ip[,2])|(ytest>pred.test.ip[,3]))

  # Plotting the graph with the prediction intervals.
  plot(pred.test.ip[, 1], ytest, pch=19, col='blue', ylim=range(pred.test.ip, ytest),
       main=" predicted values and the 95% PI versus the observed values.",
       xlab="predicted y",
       ylab="test y")
  abline(0,1)
  ic.error.bar(pred.test.ip[, 1], pred.test.ip[, 2], pred.test.ip[, 3])
  points(pred.test.ip[idx.obs.out.ip, 1], ytest[idx.obs.out.ip], pch=19, col="red")

  #print(paste("number of observed values (ùë¶ùëñ) outside the prediction interval among ", length(ytest), " observed values in total: ", length(idx.obs.out.ip)))
  #returns the percentage that the new observation is in the prediction interval
  #1 - (length(idx.obs.out.ip)/length(ytest))
}
```

En ce qui concerne la s√©lection du meilleur mod√®le, nous avons vu que le meilleur mod√®le est d√©fini comme le mod√®le avec l'erreur de pr√©diction la plus faible (RMSE) et, par cons√©quent, la MSE la plus faible. Ainsi, le k-CV, une m√©thode "d'estimation d'erreur directe", a un avantage sur l'AIC, le BIC et le R2 ajust√© vus en classe, en ce qu'il fournit une estimation directe de l'erreur de pr√©diction.

Nous en d√©duisons que parmi tous les travaux que nous avons effectu√©s, la s√©lection pas √† pas pr√©c√©dente donne le mod√®le avec le meilleur RMSE en toutes circonstances. Ce mod√®le pas √† pas devient alors notre meilleur mod√®le.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Best model
forward.step.model <- train(y ~., data = reg.data.train, method = "leapForward", 
                tuneGrid = data.frame(nvmax = 1:100), trControl = train.control)

step.predictor.names <- names(coef(forward.step.model$finalModel, 
  forward.step.model$bestTune$nvmax))[1:forward.step.model$bestTune$nvmax+1]

step.model.linreg = lm(paste("y","~", paste(step.predictor.names, collapse=" + ") ), 
                       data = reg.data.train)

ypred <- predict(step.model.linreg,newdata=reg.data.test)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
modelDiagnostic(step.model.linreg)
noquote(sprintf("Best model MSE: %.3f", MSE(reg.data.test$y,ypred)))
```

Le premier graphique repr√©sente les r√©sidus par rapport aux valeurs pr√©dites pour l'ensemble de donn√©es. La ligne bleue est un ajustement en douceur aux r√©sidus, destin√© √† cr√©er une tendance. Le deuxi√®me graphique d√©termine si les r√©sidus sont autour de z√©ro pour la plage de valeurs ajust√©es. Le dernier graphique (Q-Q des r√©sidus) montre si les r√©sidus suivent une distribution normale.

```{r echo=FALSE, message=FALSE, warning=FALSE}
modelObservations(step.model.linreg, reg.data.test)
```

Pour r√©sumer,

-   <code>summary(step.model.linreg)</code> nous montre que la plupart de nos pr√©dicteurs s√©lectionn√©s sont significatifs et que, selon la p-value (=2.2e-16 \<0.05 =\> nous rejetons le H0 hypoth√®se que tous les coefficients b√™ta j sont √©gaux √† 0), notre mod√®le est globalement significatif pour expliquer la r√©ponse variable y.

-   Les diagrammes de diagnostic du mod√®le ci-dessus montrent que le mod√®le est passable. Il y a une bonne dispersion des r√©sidus autour de z√©ro pour la plage des valeurs ajust√©es (la valeur moyenne des r√©sidus est, en fait, nulle). De plus, le graphique Q-Q des r√©sidus et l'histogramme montrent une distribution plut√¥t normale.

-   Nous observons que les intervalles de pr√©diction sont assez larges car l'intervalle de pr√©diction ùë¶0 prend en compte l'incertitude autour de lui (la variable al√©atoire) et la pr√©diction moyenne. De plus, nous pouvons voir tr√®s peu de valeurs observ√©es en dehors de l'intervalle de pr√©diction (en point rouge).

-   Le pourcentage que la nouvelle observation se trouve dans l'intervalle de pr√©diction est¬†: P \* 100.

Nous pouvons donc conclure que notre meilleur mod√®le est passable. Cependant en le testant sur le jeu de donn√©es de test du site de l'UV, nous nous sommes rendu compte qu'il overfittait les dpnn√©es. Nous sommes donc repass√©s √† un mod√®le de regression lin√©aire plus simple (prenant en compte tous les pr√©dicteurs). Cela nous a effectivement permis de faire baisser la MSE de 177 √† 157.

**II. Classification**

**A. Exploration et pr√©paration des donn√©es**

```{r include=FALSE, warning=FALSE}
classif <- read.table(file='../data/TPN1_a22_clas_app.txt', header = TRUE)
```

En explorant les donn√©es, nous observons que les colonnes de X1 √† X45 sont des pr√©dicteurs quantitatifs et les colonnes de X46 √† X50 sont des pr√©dicteurs qualitatifs. La derni√®re colonne y repr√©sente les classes √† pr√©dire (3 classes diff√©rentes). Nous d√©clarons cette colonne comme facteur contrairement aux autres colonnes qualitatives que nous comptons prendre en compte dans nos fonctions de dicrimination.

```{r}
classif$y<-as.factor(classif$y)
```

Nous avons ensuite cherch√© √† analyser les √©ventuels liens et d√©pendances entre pr√©dicteurs. En raison de la grande taille des donn√©es, il n'etait pas possible de visualiser les d√©pendances entre toutes les combinaisons possibles en m√™me temps, parmi toutes les variables existantes.

Nous utilisons donc la correlation de Pearson pour identifier s'il existe une liaison lin√©aire ou de rangement afin de diminuer le nombre de pr√©dicteurs, d'am√©liorer la pr√©cision du mod√®le et de r√©duire le temps d'ex√©cution. N'ayant pas identifi√© de corr√©lations saillantes avec Pearson, nous avons ensuite tent√© de r√©duire le nombre de variables utiles en les transformant avec une PCA. Cette PCA ne nous a pas permis non plus de r√©duire la dimension de notre probl√®me de regression.

Nous avons donc finalement d√©cid√© de commencer √† comparer nos mod√®les en prenant en compte tous les pr√©dicteurs, et reporter √† plus tard une eventuelle selection des variables. Avant de nous lancer dans les diff√©rentes m√©thodes de classification nous s√©parons enfin les donn√©es en test set (1/3 des donn√©es) et en train set (2/3 des donn√©es). Pour que les r√©sultats soient reproductibles, une seed est fix√©e (√† 1729).

```{r include=FALSE, warning=FALSE}
# separation test / train
n <- nrow(classif)
nb.train <- round(2*n/3) 
nb.test <- n - nb.train

# Training/Testing data
train <- sample(1:n, nb.train) 
classif.train <- classif[train,] 
classif.test <- classif[-train,]
```

**B. Choix d'un mod√®le de classification et optimisation de ce mod√®le**

Nous avons test√© 7 classifieurs diff√©rents : Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Regularized Discriminant Analysis (RDA), Naive Bayes (NB), Logistic Regression, Classification / Decision Tree et K-Nearest Neighbor (KNN). Les fonctions utilis√©es sont respectivement : lda, qda, rda, naive_bayes, multinom, rpart et knn.

Le but √©tait de comparer les performances obtenues avec ces diff√©rentes m√©thodes, d'optimiser les diff√©rents mod√®les et de les stabiliser avec de la cross-validation notamment. Nous avons ensuite compar√© leurs performances avec plusieurs outils : comparaison du taux d'erreur moyenn√© sur les K-fold de la cross validation, et courbes ROC multinomiales.

1.  **Tests de diff√©rents classifieurs : exemple des arbres et de la Regularized Discriminant Analysis**

Voici des extraits du code utilis√© pour la m√©thode des arbres de d√©cision :

```{r}
tree <- rpart(y~., data = classif.train, method="class",subset=train, parms = list(split = 'gini'))
rpart.plot(tree, box.palette="RdBu", shadow.col="gray",fallen.leaves=FALSE)
plotcp(tree)
idx<-which.min(tree$cptable[,"xerror"])
cp<-tree$cptable[idx,"CP"]
pruned_tree<-prune(tree,cp=cp)
# rpart.plot(pruned_tree, box.palette="RdBu", shadow.col="gray", fallen.leaves=FALSE)
pred.tree<-predict(pruned_tree,newdata=classif.test,type='class')
matrix.pred.tree<-table(classif.test$y,pred.tree)
err.tree<-1-mean(classif.test$y==pred.tree)
```

La fonction plotcp() peux tracer de l'erreur de validation crois√©e en fonction du param√®tre de complexit√©. Nous retirons la meilleure complexit√© gr√¢ce √† tree\$cptable.

Les arbres de d√©cision √©taient utiles notamment au d√©but de notre analyse, car ils nous permettaient d'observer l'importance de chaque variable gr√¢ce √† leurs forte interpretabilit√©. Nous avons ensuite tent√© d'optimiser leurs performances avec du bagging et des Random Forest, mais les performances restaient inf√©rieures √† celles obtenues avec d'autres classifieurs (Naive Bayes, QDA et RDA notamment), nous n'avons donc pas retenu cette m√©thode.

Voici des extraits du code utilis√© pour la Regularized Discriminant Analysis :

```{r}
fit.rda <- rda(y~.,data=classif.train,scale=FALSE)
pred.rda <- predict(fit.rda,newdata=classif.test) 
err.rda <- mean(classif.test$y != pred.rda$class)
```

```{r include=FALSE, warning=FALSE}
# juste pour que mon notebook fonctionne
lda.classif <- lda(y~., data=classif.train)
pred.classif.lda <- predict(lda.classif, newdata=classif.test)
err.lda <- mean(pred.classif.lda$class!=classif.test$y)

fit.naive <- naive_bayes(y~.,data=classif.train) 
pred.naive <- predict(fit.naive,newdata=classif.test) 
err.naive <- mean(pred.naive != classif.test$y) 

classif.train$y<-relevel(as.factor(classif.train$y),ref="1") # ref must be an existing level in classif$y
multi.logistreg<-multinom(y~., data=classif.train)
pred.multi.logistreg<-predict(multi.logistreg, classif.test)
err.logistreg <- mean(pred.multi.logistreg!=classif.test$y)

pred.tree.prob<-predict(tree, classif.test, "prob")
pred.multi.logistreg.prob<-predict(multi.logistreg, classif.test, "prob")
pred.nb.prob <- predict(fit.naive, newdata=classif.test, type="prob") 
```

```{r include=FALSE, warning=FALSE}
# √ßa beug 

# str(classif.train)
# str(classif.test)
# classif.train[45:50] <- lapply(classif.train[45:50], as.numeric)
# classif.test[45:50] <- lapply(classif.test[45:50], as.numeric)
# 
# fit.qda <- qda(y~.,data=classif.train)
# pred.qda <- predict(fit.qda,newdata=classif.test)
# err.qda <- mean(classif.test$y != pred.qda$class) #0.3473054
```

```{r include=FALSE, warning=FALSE}
# coder en dur
err.qda <- 0.37125748502994
```

2.  **Comparaison des performances des mod√®les : courbes ROC, et tableau r√©capitulatif des performances sur la Cross-Validation**

Nous pouvons calculer le taux d'erreur empirique en deux mani√®re, prenons LDA comme exemple:

```{r}
# moyen1
err.lda <- mean(pred.classif.lda$class!=classif.test$y)
# moyen2 en utilisant la matrice de confusion
matrix.conf.lda <- table(classif.test$y, pred.classif.lda$class)
err.lda <- 1-sum(diag(matrix.conf.lda))/nb.test
```

Donc, nous obtenons ainsi le taux d'erreur de chaque m√©thode.

```{r}
sprintf("Taux d'erreur de LDA : %f",err.lda)                                    # 0.491018
sprintf("Taux d'erreur de QDA : %f",err.qda)                                    # 0.347305
sprintf("Taux d'erreur de RDA : %f",err.rda)                                    # 0.3413174
sprintf("Taux d'erreur de Naive Bayes : %f",err.naive)                          # 0.353293
sprintf("Taux d'erreur de classification/decision trees : %f",err.tree)         # 0.4491018
sprintf("Taux d'erreur de multinomial logistic regression : %f",err.logistreg)  # 0.485030
```

Ensuite, nous affichons les courbes ROC adapt√©es √† un probl√®me de classification multinomiale en important la library "pROC".

```{r, include=FALSE, warning=FALSE}
multiroc.lda <- multiclass.roc(classif.test$y,as.vector(pred.classif.lda$posterior[,1])) 
# multiroc.qda <- multiclass.roc(classif.test$y,as.vector(pred.qda$posterior[,1])) 
multiroc.rda <- multiclass.roc(classif.test$y,as.vector(pred.rda$posterior[,1]))
multiroc.tree <- multiclass.roc(classif.test$y, as.vector(pred.tree.prob[,1])) 
multiroc.logistreg <- multiclass.roc(classif.test$y, as.vector(pred.multi.logistreg.prob[,1])) 
multiroc.nb <- multiclass.roc(classif.test$y, as.vector(pred.nb.prob[,1]))

auc(multiroc.lda)
# auc(multiroc.qda)
auc(multiroc.rda)
auc(multiroc.tree)
auc(multiroc.logistreg)
auc(multiroc.nb)

rs.lda <- multiroc.lda[['rocs']]
# rs.qda <- multiroc.qda[['rocs']]
rs.rda <- multiroc.rda[['rocs']]
rs.tree <- multiroc.tree[['rocs']]
rs.logistreg <- multiroc.logistreg[['rocs']]
rs.nb <- multiroc.nb[['rocs']]

plot.new()
plot.roc(rs.lda[[1]], col='black');plot.roc(rs.rda[[1]], col='green', add=TRUE);plot.roc(rs.tree[[1]], col='yellow', add=TRUE);plot.roc(rs.logistreg[[1]], col='blue', add=TRUE);plot.roc(rs.nb[[1]], col='purple', add=TRUE);legend("bottomright",legend=c("LDA","QDA","RDA","TREE","Logistic Regression", "NB"),col=c("black","red","green","yellow","blue","purple"),lty=1:2)

# plot.roc(rs.qda[[1]], col='red', add=TRUE);
```

<center>
   <div>
      <div>
        ![Comparaison des courbes ROC](/Users/cecileasselin/Desktop/SY19/TD4/Git_TP4_SY19/Rendu/figures/ROC.png){width="485"}
        </div>
    </div>
</center>

Test d'insertion d'image ![Comparaison des courbes ROC](/Users/cecileasselin/Desktop/SY19/TD4/Git_TP4_SY19/Rendu/figures/ROC.png)

Plus la surface sous la courbe est grande, plus la m√©thode est consid√©r√©e "performante". On affiche toutes les courbes dans la m√™me figure pour mieux comparer.

Nous pouvons donc r√©sumer que la performance des mod√®les dans ce cas-l√† est NB \> LDA \> RDA \> Logistic Regression \> Decision Tree \> QDA.

Ensuite, nous avons voulu r√©gulariser nos performances avec de la Cross-Validation. Nous avons choisi d'op√©rer avec 10 folds. Voici un exemple avec la regression lin√©aire
```{r}
# K<-10
# n<-nrow(classif)
# folds<-sample(1:K,n,replace=TRUE)
# 
# cv.err<-rep(0,10)
# 
# plot.new()
# par(mfrow=c(1,1)) 
# plot.cv.error<-function(data,x.title){
#   means.errs<-apply(data,2,mean)#mean(cv.err[,1]) # colMeans(data)
#   
#   stderr<-function(x) sd(x)/sqrt(length(x))
#   std.errs<-apply(data,2,stderr)
#   plot(data,xlab=x.title,type="b",ylab="CV.error")
# }
# 
# for(i in (1:10)){
#   for(k in 1:K){
#     fold.test<-classif[folds==k,]
#     fold.train<-classif[folds!=k,]
#     fold.classif<-lda(y~.,data=fold.train)
#     fold.predict<-predict(fold.classif,newdata=fold.test)
#     cv.err[i]<-cv.err[i]+mean(fold.predict$class!=fold.test$y)
#   } 
#   cv.err[i]<-cv.err[i]/n
# }
# plot.cv.error(cv.err,x.title = "LDA - 10 crossvalidation")
```

En comparant le taux d'erreur moyen obtenu sur les 10 folds pour chaque mod√®le, nous en venons √† la conclusion que RDA est toujours le meilleur mod√®le

Choisissez un mod√®le Selon la performance du taux d'erreur, de la courbe ROC et de la k-fold cross-validation de tous les 7 mod√®les ci-dessus, nous choisissons enfin la mod√®le RDA(Regularized discriminant analysis) pour classifier les donn√©es.

```{r}
### codes complets pour RDA??
```

############# 

**3. Analyse des r√©sultats** Nous avons enfin effectu√© le test de Mc Nemar pour √©valuer la significativit√© des ecarts de performance.

############ Archives

Test d'insertion d'image ![Ceci est un test](/Users/cecileasselin/Desktop/SY19/TD4/test_plot.png)

Voici les codes pour la m√©thode kNN:

```{r}
# for(k in 1:30){
#   classifier_knn <- knn(train = train_scale,
#                         test = test_scale,
#                         cl = classif.train$y,
#                         k = k)
#   misClassError <- mean(classifier_knn != classif.test$y)
#   print(paste(k, '- Accuracy =', 1-misClassError))
#   err.knn <- min(err.knn, misClassError)
#   plot(k,1-misClassError)
# }
# print(paste('maxAccuracy =', 1-err.knn))
```

figure !!!!
