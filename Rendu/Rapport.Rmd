---
title: "Rapport TP4 SY19 GR D2P1G"
output:
  pdf_document: 
    latex_engine: xelatex
  html_notebook: default
---

**Rapport de projet 1 SY19 A22 - Sun Jingwen, Ho Nhu Hoang, Asselin Cecile**\
\
**Sommaire**

I. Introduction

II\. Regression

1.  Exploration et préparation des données
2.  Choix d'un modèle de regression et optimisation de ce modèle
3.  Analyse des résultats

III\. Classification

1.  Exploration et préparation des données
2.  Choix d'un modèle de classification et optimisation de ce modèle
3.  Analyse des résultats

I. Introduction

Nous chargeons les différents packages et les données nécessaires à l'étude.

```{r, echo=FALSE}
#Regression
library(gridExtra)
library(ggplot2)
library(tidyverse)
library(caret)
library(leaps)
library(MASS)
library(glmnet)
library(gam)

reg.data <- read.table('../data/TPN1_a22_reg_app.txt', header=TRUE)
classif <- read.table('../data/TPN1_a22_clas_app.txt', header=TRUE)
```

Nous pouvons analyser les données de régression et les données de classification respectivement contenues dans la variable <code>reg.data</code> et la variable <code>classif</code>.

<u>I. ***REGRESSION:***</u>

**A.** **Exploration et préparation des donnée**

La première étape de l'étude de régression consiste à explorer les données.

```{r}
summary.data = as.data.frame(apply(reg.data, 2, summary))
summary.data
n = nrow(reg.data)
p = ncol(reg.data) - 1
matplot(t(summary.data[3,-length(summary.data)]),
        type = "l",
        main = "Median",
        ylab = "value")

```

Nous regardons que: - La taille de l'échantillon n = 500 et le nombre de prédicteurs : p = 100. Nous en déduisons donc que notre jeu de données est de grande dimension et n\>p, ce qui sera fondamental dans la partie modèle. - En regardant les statistiques récapitulatives (<code>summary.data</code>) des différents prédicteurs, nous comprenons que tous ont une valeur médiane comprise entre 4,7 et 5,29 avec une valeur échelonnée de 0 à 10. Ils ont donc distributions identiques, il n'est donc pas nécessaire de mettre à l'échelle le prédicteur de données.

**1. Préparation de donnée** Dans un premier temps, nous évaluerons rapidement l'ensemble du modèle dans sa complexité, sans chercher à l'améliorer ou à le simplifier, en appliquant une simple régression linéaire. Et pour ce faire, il faut préparer les données en les divisant aléatoirement en un ensemble d'apprentissage (66% pour construire un modèle prédictif) et un ensemble de validation (34% pour évaluer le modèle).

```{r}
########### training set and validation set ################
set.seed(1729) # the Hardy CRamanujan number

rows <- nrow(reg.data)
cols <- ncol(reg.data) - 1
train_size <- 2 / 3
nb_train <- round(train_size * rows)
nb_test <- rows - nb_train

# training set
train <- sample(1:rows, nb_train)
reg.data.train <- reg.data[train,]
# validation set
reg.data.test  <- reg.data[-train,]
```

\*Nous créons une fonction pour obtenir l'erreur quadratique moyenne des différents modèles que nous allons évaluer et comparer pour trouver le meilleur : <u>MSE</u>. 2 arguments : -y_predict : le y prédit par le modèle avec l'observation de test. -y_test : la réponse y des données de test correspondant aux observations de test.

```{r}
# MSE
MSE = function(y_test,y_predict){
  mean((y_test-y_predict)^2)
}
```

```{r}
full.model.reg <- lm(y~., data = reg.data.train)
summary(full.model.reg)
predictions <- predict(full.model.reg, newdata = reg.data.test)
full.model.reg.mse <- MSE(reg.data.test$y, predictions)
plot(reg.data.test$y, predictions)
abline(0,1)
full.model.reg.mse
```

Cette première étude montre que le modèle tel qu'il inclut tous les prédicteurs : Xi i = 1,..., 100 génère déjà une bonne régression linéaire multidimensionnelle pour prédire y. L'idée est maintenant de déterminer un meilleur modèle pour obtenir la meilleure régression possible. Il est important de préciser que pour la comparaison de nos modèles mais aussi l'estimation des hyperparamètres de certains modèles, nous utiliserons principalement la kfold-cross-validation (k-CV). En ce qui concerne la sélection du meilleur modèle, nous avons vu que le meilleur modèle est défini comme le modèle avec l'erreur de prédiction la plus faible (RMSE) et, par conséquent, la MSE la plus faible. Ainsi, le k-CV, une méthode "d'estimation d'erreur directe", a un avantage sur l'AIC, le BIC et le R2 ajusté vus en classe, en ce qu'il fournit une estimation directe de l'erreur de prédiction. En outre, il peut également être utilisé dans un plus large éventail de tâches de sélection de modèles. Comme chaque ensemble d'apprentissage est seulement (K - 1)/K aussi grand que l'ensemble d'apprentissage d'origine, les estimations d'erreur de prédiction seront généralement biaisées vers le haut. Ce biais est minimisé lorsque K = n (LOOCV), mais cette estimation a une variance élevée, car les estimations pour chaque pli sont fortement corrélées. K = 5, compte tenu de notre grande taille d'échantillon n, fournit un bon compromis pour ce compromis biais-variance.

```{r}
train.control <- trainControl(method = "cv", number = 5)
```

Passons maintenant à la sélection du modèle : Pour cela, nous allons passer en revue toutes les méthodes vues en cours pour trouver les meilleurs modèles et les tester pour voir s'ils « correspondent » à notre jeu de données. Pour cela, nous allons générer un 5-CV en utilisant toutes les données fournies, dans lequel à chaque itération, k : Pour chaque méthode de déduction du modèle optimisé : -Nous allons d'abord estimer les paramètres de réglage du modèle associé à la méthode actuelle en utilisant tous les plis sauf le pli k. -A partir de là, nous obtiendrons les meilleurs paramètres de réglage associés à la méthode actuelle. -Enfin, nous allons réestimer le modèle en utilisant tous les plis sauf le pli k, et nous l'évaluons avec le pli k en calculant son MSE, que nous stockons dans la table des cv_errors correspondant à la méthode courante.

Enfin, nous allons calculer le cv_error moyen pour chaque modèle obtenu et sélectionner celui avec le plus petit cv_error.

Cette procédure est, à notre avis, celle qui offre les résultats les moins biaisés.

*\*important note:* La méthode de sélection du meilleur sous-ensemble est très puissante mais, pour des raisons de calcul, elle ne peut pas être appliquée ici car p est trop grand. De plus, la sélection du meilleur sous-ensemble, même si elle était possible ici, peut également souffrir de problèmes statistiques lorsque p est grand : plus l'espace de recherche est grand, plus il est probable de trouver des modèles qui semblent bons sur les données d'apprentissage, même s'ils ont aucun pouvoir prédictif sur les données futures. Ainsi, un espace de recherche énorme peut conduire à un surajustement et à une variance élevée des estimations de coefficients. Pour ces deux raisons, les méthodes pas à pas explorant un ensemble beaucoup plus restreint de modèles représentent ici de très bonnes alternatives à la meilleure sélection de sous-ensemble que nous allons donc mettre en œuvre. De plus, après avoir vu en classe que les méthodes des plus proches voisins peuvent mal fonctionner lorsque p est grand car les plus proches voisins ont tendance à être éloignés en grandes dimensions, nous avons décidé de ne pas les utiliser dans le CV suivant en raison du grand nombre de prédicteurs disponibles ici.

**2. Choix d'un modèle et l'optimisation** On utilise les méthodes suivants: Stepwise selection: <u>1 Forward selection</u> <u>2 Backward selection</u>

Penelized regression: <u>1 Ridge regression</u> <u>2 Lasso regression</u> <u>3 Elasticnet regression</u>

```{r}
# 10-CV set up:
kfolds <- 10
n <- nrow(reg.data)
p <- ncol(reg.data)
ntst <- n/kfolds

## Define fold_ids
fold_ids      <- rep(seq(kfolds), ceiling(n / kfolds))
fold_ids      <- fold_ids[1:n]
fold_ids      <- sample(fold_ids, length(fold_ids))

## Initialize vectors to store CV errors
Elastic_CV_MSE_vec  <- vector(length = kfolds, mode = "numeric")
Lasso_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Ridge_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Forward_step_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Backward_step_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")
Full_CV_MSE_vec  <-vector(length = kfolds, mode = "numeric")

#10-CV execution
## Loop through the folds:
for (k in 1:kfolds){

  ############## STEPWISE SELECTION #############

  forward.step.model <- train(y ~., data = reg.data[which(fold_ids != k),],
                            method = "leapForward",
                            tuneGrid = data.frame(nvmax = 1:100),
                            trControl = train.control
  )
  step.predictor.names <- names(coef(forward.step.model$finalModel, forward.step.model$bestTune$nvmax))[1:forward.step.model$bestTune$nvmax+1]

  step.model.linreg = lm(paste("y","~", paste(step.predictor.names, collapse=" + ") ), data = reg.data[which(fold_ids != k),])
  predictions <- predict(step.model.linreg,newdata=reg.data[which(fold_ids == k),])

  Forward_step_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  backward.step.model <- train(y ~., data = reg.data[which(fold_ids != k),],
                            method = "leapBackward",
                            tuneGrid = data.frame(nvmax = 1:100),
                            trControl = train.control
  )
  step.predictor.names <- names(coef(backward.step.model$finalModel, backward.step.model$bestTune$nvmax))[1:backward.step.model$bestTune$nvmax+1]

  step.model.linreg = lm(paste("y","~", paste(step.predictor.names, collapse=" + ") ), data = reg.data[which(fold_ids != k),])
  predictions <- predict(step.model.linreg,newdata=reg.data[which(fold_ids == k),])

  Backward_step_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  ############# PENALIZED REGRESSION ###########
  x <- model.matrix(y~., reg.data[which(fold_ids != k),-p-1])[,-1]

  y <- reg.data[which(fold_ids != k),]$y
  x.test <- model.matrix(y~., reg.data[which(fold_ids == k),-p-1])[,-1]

  ########## ELASTICNET REGRESSION #############
  elastic <- train(y ~., data = reg.data[which(fold_ids != k),],
                   method = "glmnet",
                   trControl = train.control,
                   tuneLength = 10)

  elastic.alpha <- elastic$bestTune[1,1]
  elastic.lambda <- elastic$bestTune[1,2]

  fit.elsatic <- glmnet(x, reg.data[which(fold_ids != k),p], lambda=elastic.lambda, alpha = elastic.alpha)
  predictions  <- predict(fit.elsatic, x.test)

  Elastic_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  ########## RIDGE REGRESSION #############
  cv.ridge <- cv.glmnet(x, y, alpha = 0)

  fit.ridge <- glmnet(x, reg.data[which(fold_ids != k),p], lambda=cv.ridge$lambda.min, alpha = 0)
  predictions  <- predict(fit.ridge, x.test)
  Ridge_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  ########## LASSO REGRESSION #############
  cv.lasso <- cv.glmnet(x, y, alpha = 1)
  fit.lasso <- glmnet(x, reg.data[which(fold_ids != k),p], lambda=cv.lasso$lambda.min, alpha = 1)
  predictions  <- predict(fit.lasso, x.test)
  Lasso_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

  #REGRESSION OBTAINED W/ THE FULL MODEL (NO SELECTION)
  model.linreg = lm(y~.,  data = reg.data[which(fold_ids != k),])
  predictions <- predict(model.linreg,newdata=reg.data[which(fold_ids == k),])

  Full_CV_MSE_vec[k] <- MSE(reg.data[which(fold_ids == k),p], predictions)

}
```

Nous sommes maintenant en mesure de comparer les résultats des performances de notre modèle.

```{r}
noquote(sprintf("forward step regression model MSE: %.3f",mean(Forward_step_CV_MSE_vec)))
noquote(sprintf("backward step regression MSE: %.3f",mean(Backward_step_CV_MSE_vec)))
noquote(sprintf("elastic net model MSE: %.3f",mean(Elastic_CV_MSE_vec)))
noquote(sprintf("lasso net model MSE: %.3f",mean(Lasso_CV_MSE_vec)))
noquote(sprintf("ridge model MSE: %.3f",mean(Ridge_CV_MSE_vec)))
noquote(sprintf("full model regression MSE: %.3f",mean(Full_CV_MSE_vec)))
```

Nous en déduisons que parmi tous les travaux que nous avons effectués, la sélection pas à pas précédente donne le modèle avec le meilleur RMSE en toutes circonstances. Ce modèle pas à pas devient alors notre meilleur modèle.

**Analyse des résultats** La dernière étape de cette étude de régression sera d'analyser et d'évaluer le modèle de régression que nous pouvons obtenir avec le meilleur modèle trouvé : Pour obtenir l'erreur de test la moins biaisée pour notre meilleur modèle de prédiction, nous procéderons comme suit : nous réestimerons les paramètres du modèle à l'aide de l'ensemble d'apprentissage. Et pour obtenir une estimation impartiale de la meilleure erreur de modèle, nous utiliserons notre ensemble de validation créé au début comme ensemble de test indépendant.

\*Nous avons créé des fonctions pour analyser et évaluer le meilleur modèle que nous avons trouvé pour la régression : - <u>modelDiagnostic :</u> Cette fonction implémente 3 tracés résiduels utiles par rapport à notre modèle : -\> le premier graphique représente les résidus par rapport aux valeurs prédites pour l'ensemble de données. La ligne bleue est un ajustement en douceur aux résidus, destiné à créer une tendance. -\> le deuxième graphique détermine si les résidus sont autour de zéro pour la plage de valeurs ajustées. -\> le dernier graphique (graphique Q-Q des résidus) montre si les résidus suivent une distribution normale. - <u>modelObservations :</u> cette fonction représente graphiquement la prédiction et l'intervalle de prédiction en fonction des valeurs de test. elle renvoie le pourcentage de la nouvelle observation dans l'intervalle de prédiction.

```{r}
modelDiagnostic <- function(model.reg)
{
  pMod <- fortify(model.reg)

  # the residuals vs predicted values
  p1 <- ggplot(pMod, aes(x=.fitted, y=.resid))+geom_point() +
    geom_smooth(se=FALSE)+geom_hline(yintercept=0, col="red", linetype="dashed") +
    xlab("Fitted Values")+ylab("Residuals") +
    ggtitle("Residual vs Fitted Plot")

  pMod$.qqnorm <- qqnorm(pMod$.stdresid, plot.it=FALSE)$x
  y <- quantile(pMod$.stdresid, c(0.25, 0.75))   # Find the 1st and 3rd quartiles
  x <- quantile(pMod$.qqnorm  , c(0.25, 0.75))   # Find the 1st and 3rd quartiles
  slope <- diff(y) / diff(x)                     # Compute the line slope
  int <- y[1] - slope * x[1]                     # Compute the line intercept

  # Create residuals QQ plot.
  p2 <- ggplot(pMod, aes(.qqnorm, .stdresid)) +
    geom_point(na.rm = TRUE) +
    geom_abline(intercept=int, slope=slope, color="red")           +
    xlab("Theoretical Quantiles") + ylab("Standardized Residuals") +
    ggtitle("Normal Q-Q Plot")

  # Create residuals histogram plot.
  p3 <- ggplot(data=pMod, aes(x=.resid)) +
    geom_histogram(binwidth=0.5, fill="blue") +
    xlab("Residuals") +
    ggtitle("Distribution of Residuals")

  grid.arrange(p1, p3, p2, nrow = 1 , top="Model Diagnostic Plots")
}

modelObservations <- function(model.reg, data.test){
  #plotting ytest and ypred
  ypred <- predict(model.reg,data.test)
  plot(data.test$y,ypred, main="predicted values versus test values",
       xlab="predicted y",
       ylab="test y")
  abline(0,1)

  # getting the observations outside the prediction interval
  ic.error.bar <- function(x, lower, upper, length=0.1){
    arrows(x, upper, x, lower, angle=90, code=3, length=length)
  }

  #Calculate the prediction intervals of 𝑦0 (the predicted value), with the predict command using the interval = "prediction" option
  pred.test.ip <- predict(model.reg, interval='prediction', newdata = data.test)
  ytest <- data.test$y

  # get the observations being outside the prediction interval
  idx.obs.out.ip <- which((ytest<pred.test.ip[,2])|(ytest>pred.test.ip[,3]))

  # Plotting the graph with the prediction intervals.
  plot(pred.test.ip[, 1], ytest, pch=19, col='blue', ylim=range(pred.test.ip, ytest),
       main=" predicted values and the 95% PI versus the observed values.",
       xlab="predicted y",
       ylab="test y")
  abline(0,1)
  ic.error.bar(pred.test.ip[, 1], pred.test.ip[, 2], pred.test.ip[, 3])
  points(pred.test.ip[idx.obs.out.ip, 1], ytest[idx.obs.out.ip], pch=19, col="red")

  print(paste("number of observed values (𝑦𝑖) outside the prediction interval among ", length(ytest), " observed values in total: ", length(idx.obs.out.ip)))
  #returns the percentage that the new observation is in the prediction interval
  1 - (length(idx.obs.out.ip)/length(ytest))
}
```

```{r}
# Best model
forward.step.model <- train(y ~., data = reg.data.train,
                          method = "leapForward",
                          tuneGrid = data.frame(nvmax = 1:100),
                          trControl = train.control
)

step.predictor.names <- names(coef(forward.step.model$finalModel, forward.step.model$bestTune$nvmax))[1:forward.step.model$bestTune$nvmax+1]

step.model.linreg = lm(paste("y","~", paste(step.predictor.names, collapse=" + ") ), data = reg.data.train)
summary(step.model.linreg)
ypred <- predict(step.model.linreg,newdata=reg.data.test)

noquote(sprintf("Best model MSE: %.3f", MSE(reg.data.test$y,ypred)))

modelDiagnostic(step.model.linreg)
P <- modelObservations(step.model.linreg, reg.data.test)
```

Premièrement, <code>summary(step.model.linreg)</code> nous montre que la plupart de nos prédicteurs sélectionnés sont significatifs et que, selon la p-value (=2.2e-16 \<0.05 =\> nous rejetons le H0 hypothèse que tous les coefficients bêta j sont égaux à 0), notre modèle est globalement significatif pour expliquer la réponse variable y. Deuxièmement, les diagrammes de diagnostic du modèle ci-dessus montrent que le modèle est passable. Il y a une bonne dispersion des résidus autour de zéro pour la plage des valeurs ajustées (la valeur moyenne des résidus est, en fait, nulle). De plus, le graphique Q-Q des résidus et l'histogramme montrent une distribution plutôt normale. Troisièmement, nous observons que les intervalles de prédiction sont assez larges car l'intervalle de prédiction 𝑦0 prend en compte l'incertitude autour de lui (la variable aléatoire) et la prédiction moyenne. De plus, nous pouvons voir très peu de valeurs observées (𝑦𝑖) en dehors de l'intervalle de prédiction (en point rouge). Enfin, le pourcentage que la nouvelle observation se trouve dans l'intervalle de prédiction est :

```{r}
P * 100
```

Nous pouvons donc conclure que notre meilleur modèle est passable et que sa prédiction est très bonne pour un jeu de données de test indépendant du jeu de données qui a été utilisé pour construire et entraîner notre modèle. Nous sommes donc satisfaits de notre étude et prêts à utiliser notre meilleur modèle pour le jour des évaluations.

**II. Classification**

**1. Présentation des données** **2. Choix d'un modèle de classification et optimisation de ce modèle 3. Analyse des résultats**

```{r}
# code KNN
# library(nnet)
# fit<-multinom(y~., data=classif.train)
# pred.classif<-predict(fit, newdata=classif.test)
# perf<-table(classif.test$y, pred.classif)
# err.reglog <- 1-sum(diag(perf))/nb.test
```

Test d'insertion d'image ![Ceci est un test](/Users/cecileasselin/Desktop/SY19/TD4/test_plot.png)

```{r}
# head()
```

```{r setup}
library(r2d3)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
